{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy dataset\n",
    "seq_length = 10  # Sequence-length\n",
    "feature_dim = 16  # Feature-dimension\n",
    "\n",
    "# Dummy input data: batch_size x seq_length x feature_dim\n",
    "batch_size = 32\n",
    "dummy_data = torch.randn(batch_size, seq_length, feature_dim)\n",
    "\n",
    "# Dummy target data: batch_size x seq_length x 1\n",
    "dummy_targets = torch.randn(batch_size, seq_length, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a learnable positional encoding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_length, feature_dim):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(seq_length, feature_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding.unsqueeze(0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, seq_length, feature_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.positional_encoding = LearnablePositionalEncoding(seq_length, feature_dim)\n",
    "        self.fc = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2348\n",
      "Epoch [2/10], Loss: 2.2168\n",
      "Epoch [3/10], Loss: 2.1990\n",
      "Epoch [4/10], Loss: 2.1814\n",
      "Epoch [5/10], Loss: 2.1639\n",
      "Epoch [6/10], Loss: 2.1467\n",
      "Epoch [7/10], Loss: 2.1297\n",
      "Epoch [8/10], Loss: 2.1128\n",
      "Epoch [9/10], Loss: 2.0962\n",
      "Epoch [10/10], Loss: 2.0797\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "model = SimpleModel(seq_length, feature_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(dummy_data)\n",
    "    loss = criterion(outputs, dummy_targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8273],\n",
      "         [-0.7170],\n",
      "         [ 0.0500],\n",
      "         [-1.2644],\n",
      "         [ 1.0572],\n",
      "         [ 0.2625],\n",
      "         [-0.4544],\n",
      "         [ 0.0724],\n",
      "         [-1.3131],\n",
      "         [ 0.7555]],\n",
      "\n",
      "        [[-0.3741],\n",
      "         [-0.4205],\n",
      "         [-0.8019],\n",
      "         [-1.7189],\n",
      "         [ 1.1025],\n",
      "         [ 0.9819],\n",
      "         [-0.6126],\n",
      "         [-1.4597],\n",
      "         [-1.5803],\n",
      "         [ 0.7580]],\n",
      "\n",
      "        [[-0.3750],\n",
      "         [ 0.0768],\n",
      "         [-0.1298],\n",
      "         [-0.6686],\n",
      "         [ 0.4856],\n",
      "         [ 0.2013],\n",
      "         [ 0.2197],\n",
      "         [-0.6843],\n",
      "         [-1.4695],\n",
      "         [ 0.8565]],\n",
      "\n",
      "        [[ 0.5397],\n",
      "         [ 1.0832],\n",
      "         [ 1.1116],\n",
      "         [-2.3337],\n",
      "         [-0.4876],\n",
      "         [-0.3035],\n",
      "         [-1.9440],\n",
      "         [-0.5994],\n",
      "         [-2.1460],\n",
      "         [ 0.5176]],\n",
      "\n",
      "        [[ 0.1136],\n",
      "         [-0.5672],\n",
      "         [ 0.2014],\n",
      "         [-2.3913],\n",
      "         [-0.7556],\n",
      "         [-0.0776],\n",
      "         [-1.1258],\n",
      "         [-0.7405],\n",
      "         [-1.3143],\n",
      "         [ 1.3490]],\n",
      "\n",
      "        [[-0.4047],\n",
      "         [-0.7997],\n",
      "         [-0.0945],\n",
      "         [-0.7551],\n",
      "         [ 1.4549],\n",
      "         [-0.3005],\n",
      "         [-0.1288],\n",
      "         [-2.0810],\n",
      "         [-0.8223],\n",
      "         [ 1.1078]],\n",
      "\n",
      "        [[-0.0926],\n",
      "         [-1.1826],\n",
      "         [-0.0998],\n",
      "         [-1.9890],\n",
      "         [ 0.6400],\n",
      "         [-0.6109],\n",
      "         [-1.5172],\n",
      "         [-1.5009],\n",
      "         [-1.5370],\n",
      "         [ 0.3763]],\n",
      "\n",
      "        [[-0.7625],\n",
      "         [ 0.5815],\n",
      "         [-0.6974],\n",
      "         [-1.3761],\n",
      "         [-0.8174],\n",
      "         [-0.9110],\n",
      "         [-1.1759],\n",
      "         [-0.3808],\n",
      "         [-0.9620],\n",
      "         [ 0.2325]],\n",
      "\n",
      "        [[-0.4225],\n",
      "         [-0.1409],\n",
      "         [-1.4215],\n",
      "         [-1.6241],\n",
      "         [ 1.2485],\n",
      "         [ 0.2935],\n",
      "         [-0.6976],\n",
      "         [-1.7552],\n",
      "         [-0.9954],\n",
      "         [ 1.7656]],\n",
      "\n",
      "        [[-0.3594],\n",
      "         [ 0.3669],\n",
      "         [-0.2602],\n",
      "         [-2.4080],\n",
      "         [ 0.0296],\n",
      "         [-0.5550],\n",
      "         [-0.6313],\n",
      "         [-1.5130],\n",
      "         [-0.5929],\n",
      "         [ 0.9611]],\n",
      "\n",
      "        [[-0.5975],\n",
      "         [-0.3806],\n",
      "         [-0.4365],\n",
      "         [-0.9885],\n",
      "         [ 0.9317],\n",
      "         [-0.7152],\n",
      "         [-0.9462],\n",
      "         [-0.5132],\n",
      "         [-1.0679],\n",
      "         [ 0.8617]],\n",
      "\n",
      "        [[ 0.2312],\n",
      "         [ 1.2063],\n",
      "         [-0.5618],\n",
      "         [-2.4125],\n",
      "         [ 1.3304],\n",
      "         [-0.3357],\n",
      "         [-1.3934],\n",
      "         [-0.5479],\n",
      "         [-0.5750],\n",
      "         [ 0.4837]],\n",
      "\n",
      "        [[-0.9390],\n",
      "         [-0.8734],\n",
      "         [-0.1927],\n",
      "         [-0.8196],\n",
      "         [ 1.8182],\n",
      "         [ 0.0835],\n",
      "         [-0.5337],\n",
      "         [-1.5940],\n",
      "         [-1.0442],\n",
      "         [-0.5163]],\n",
      "\n",
      "        [[-0.0966],\n",
      "         [-1.6215],\n",
      "         [-0.3108],\n",
      "         [-1.5839],\n",
      "         [-0.1029],\n",
      "         [ 0.5449],\n",
      "         [-1.2333],\n",
      "         [-1.2315],\n",
      "         [-1.1489],\n",
      "         [-0.3512]],\n",
      "\n",
      "        [[ 0.4439],\n",
      "         [ 0.5128],\n",
      "         [-0.1837],\n",
      "         [-1.7894],\n",
      "         [ 0.8793],\n",
      "         [-0.2182],\n",
      "         [-2.1451],\n",
      "         [-0.5699],\n",
      "         [-0.7973],\n",
      "         [-1.0307]],\n",
      "\n",
      "        [[-1.0339],\n",
      "         [-1.0354],\n",
      "         [-1.0050],\n",
      "         [-1.3396],\n",
      "         [ 0.3607],\n",
      "         [-1.2487],\n",
      "         [-1.9102],\n",
      "         [-1.4448],\n",
      "         [-0.3236],\n",
      "         [ 0.2477]],\n",
      "\n",
      "        [[-0.4054],\n",
      "         [-0.4420],\n",
      "         [-1.0093],\n",
      "         [-1.2990],\n",
      "         [ 1.3055],\n",
      "         [ 0.5853],\n",
      "         [-0.5433],\n",
      "         [-0.2525],\n",
      "         [-0.1358],\n",
      "         [-0.3471]],\n",
      "\n",
      "        [[-0.7116],\n",
      "         [-1.1321],\n",
      "         [-1.9462],\n",
      "         [-2.7844],\n",
      "         [ 0.4815],\n",
      "         [-0.3617],\n",
      "         [-1.2228],\n",
      "         [ 0.0096],\n",
      "         [-2.4360],\n",
      "         [ 0.4013]],\n",
      "\n",
      "        [[-0.3954],\n",
      "         [ 0.4903],\n",
      "         [-0.1542],\n",
      "         [-0.6452],\n",
      "         [ 0.1448],\n",
      "         [-0.2689],\n",
      "         [-0.9120],\n",
      "         [-2.2166],\n",
      "         [-1.0446],\n",
      "         [ 0.1785]],\n",
      "\n",
      "        [[-0.0622],\n",
      "         [ 0.3286],\n",
      "         [-0.0220],\n",
      "         [-2.3089],\n",
      "         [ 0.7083],\n",
      "         [ 0.4074],\n",
      "         [-0.9512],\n",
      "         [-0.6784],\n",
      "         [-1.1709],\n",
      "         [-0.3983]],\n",
      "\n",
      "        [[ 0.1020],\n",
      "         [ 0.1671],\n",
      "         [-0.3090],\n",
      "         [-1.4429],\n",
      "         [ 0.6061],\n",
      "         [ 0.7112],\n",
      "         [-0.9400],\n",
      "         [-2.8246],\n",
      "         [-0.9349],\n",
      "         [ 0.6537]],\n",
      "\n",
      "        [[-0.2410],\n",
      "         [-0.9782],\n",
      "         [-0.4981],\n",
      "         [-1.6774],\n",
      "         [ 0.3051],\n",
      "         [ 0.3210],\n",
      "         [-0.8588],\n",
      "         [-1.8664],\n",
      "         [-0.6592],\n",
      "         [ 0.7923]],\n",
      "\n",
      "        [[-0.4848],\n",
      "         [-0.7987],\n",
      "         [-0.2809],\n",
      "         [-2.0522],\n",
      "         [ 1.5208],\n",
      "         [ 1.0513],\n",
      "         [-0.5959],\n",
      "         [-1.2817],\n",
      "         [ 1.0408],\n",
      "         [ 0.9201]],\n",
      "\n",
      "        [[-0.5003],\n",
      "         [-0.3012],\n",
      "         [-0.1736],\n",
      "         [ 0.1224],\n",
      "         [ 0.9049],\n",
      "         [ 0.0456],\n",
      "         [-0.4569],\n",
      "         [-0.5797],\n",
      "         [-0.5468],\n",
      "         [ 0.9974]],\n",
      "\n",
      "        [[ 0.1581],\n",
      "         [ 0.6261],\n",
      "         [ 0.1388],\n",
      "         [-0.9067],\n",
      "         [ 0.8560],\n",
      "         [-0.2285],\n",
      "         [-0.5350],\n",
      "         [-0.2954],\n",
      "         [-1.3185],\n",
      "         [-0.0726]],\n",
      "\n",
      "        [[-0.8596],\n",
      "         [-1.6207],\n",
      "         [-0.2758],\n",
      "         [-1.7585],\n",
      "         [ 0.5302],\n",
      "         [ 0.5599],\n",
      "         [-1.3276],\n",
      "         [-0.6845],\n",
      "         [-0.7140],\n",
      "         [ 0.8122]],\n",
      "\n",
      "        [[-0.2610],\n",
      "         [ 0.0356],\n",
      "         [-0.2293],\n",
      "         [-1.4350],\n",
      "         [-0.0165],\n",
      "         [ 0.7502],\n",
      "         [-0.4906],\n",
      "         [-1.0923],\n",
      "         [-2.9813],\n",
      "         [ 0.9140]],\n",
      "\n",
      "        [[-0.2756],\n",
      "         [-1.0795],\n",
      "         [-0.9602],\n",
      "         [-1.8399],\n",
      "         [ 0.9949],\n",
      "         [-0.1028],\n",
      "         [-0.1680],\n",
      "         [-1.4911],\n",
      "         [-0.6508],\n",
      "         [ 0.1389]],\n",
      "\n",
      "        [[ 0.1885],\n",
      "         [-0.0755],\n",
      "         [-0.2551],\n",
      "         [-1.2946],\n",
      "         [ 1.2826],\n",
      "         [ 0.7788],\n",
      "         [-1.0522],\n",
      "         [-1.0568],\n",
      "         [-0.4648],\n",
      "         [-0.0239]],\n",
      "\n",
      "        [[-0.1053],\n",
      "         [-0.8966],\n",
      "         [-0.6783],\n",
      "         [-2.5804],\n",
      "         [ 0.5681],\n",
      "         [-0.5834],\n",
      "         [-0.1300],\n",
      "         [-1.3057],\n",
      "         [-0.7702],\n",
      "         [ 0.0618]],\n",
      "\n",
      "        [[ 0.1866],\n",
      "         [-1.0053],\n",
      "         [-1.0560],\n",
      "         [-2.1661],\n",
      "         [ 0.2543],\n",
      "         [ 0.5462],\n",
      "         [-0.9294],\n",
      "         [-0.9444],\n",
      "         [-1.1387],\n",
      "         [ 0.2240]],\n",
      "\n",
      "        [[-0.1976],\n",
      "         [-0.6897],\n",
      "         [ 0.9668],\n",
      "         [-2.4935],\n",
      "         [ 0.0037],\n",
      "         [ 0.2130],\n",
      "         [-1.9734],\n",
      "         [-0.2609],\n",
      "         [-0.2153],\n",
      "         [ 0.4360]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate new dummy data for evaluation\n",
    "new_dummy_data = torch.randn(batch_size, seq_length, feature_dim)\n",
    "new_outputs = model(new_dummy_data)\n",
    "print(new_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
